Convolutional Neural Network (CNN) w/ Tensorflow (1/2)
===
### Taehun Kim

---

Why CNN?
===

- Traditional NN failed!
- Need to reduce the number of parameters
  - e.g. 32\*32\*3 = 3072, 200\*200\*3 = 120,000
  - To make training scalable
  - To avoid overfitting
- Need to consider spatial structure of data
  - up and down, near and far, position,...

---
CNN & Features of CNN
===
- CNN is a NN that mimics animal visual cortex 
- Input is an ___image___ (3d = width \* height \* depth)
  - depth is usually called (input) channel
  - e.g. RGB image, depth = 3 
- Local connectivity: input and output are not fully connected
- Shared weights: one filter, one feature, one weights

---
Components of CNN architecture
===

- Convolutional Layer: filter the input and make filtered output
- Pooling Layer: reduce dim. of input(s)
- Fully-connected(FC) Layer: learn from every feature extracted

---

Quick view of tensorflow
===
- TensorFlow is an open source software library for numerical computation using data flow graphs 
- Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them

1. import tensorflow
2. generate graphs
    - model parameters, input, output, loss function, optimizer,...
3. generate and run the session

### Example

#### Basic graph
```python
import tensorflow as tf

node1 = tf.constant(3.0, tf.float32)
node2 = tf.constant(4.0) # also tf.float32 implicitly
print(node1, node2)
```
will print
> Tensor("Const:0", shape=(), dtype=float32) Tensor("Const_1:0", shape=(), dtype=float32)

#### Running session
```python
sess = tf.Session()
print(sess.run([node1, node2]))
```
will print
> [3.0, 4.0]

#### Adding operations
```python
node3 = tf.add(node1, node2)
print("node3: ", node3)
print("sess.run(node3): ",sess.run(node3))
```
will print
> node3:  Tensor("Add_2:0", shape=(), dtype=float32)

> sess.run(node3):  7.0

The detail will be covered next time!
Let's move to cnn..!

---

Convolutional Layer
===

### tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)

- Args.
  - input: X (e.g. m\*32\*32\*3)
  - filter: W (e.g. 5\*5\*3\*n)
  - strides: [1, val1, val2, 1]
  - padding: 'SAME', 'VALID'
    - SAME: try to pad zero(s) evenly
    - e.g. 1 2 3 4--> 0 1 2, 2 3 4, 4 0 0   
    - VALID: drop rightmost cols
    - e.g. 1 2 3 4--> 1 2 3, _3_ _4_ (dropped) 
  - use\_cudnn\_on\_gpu, data_format, name: optional
- Return
  - output[b, i, j, k] =
    sum_{di, dj, q} ( input[b, strides[1] * i + di, strides[2] * j + dj, q] * filter[di, dj, q, k])

### Example

- x = tf.constant([[1., 2., 3.,],
                 [4., 5., 6.],
                 [7., 8., 9.]]) 
- w = tf.constant([[1., 0.],[0., 1.]])
- strides = [1, 1, 1, 1]
- padding = p ('VALID' or 'SAME')
```python
x = tf.constant([[1., 2., 3.,], [4., 5., 6.], [7., 8., 9.]])
w = tf.constant([[1., 0.],[0., 1.]])
strds= [1,1,1,1]

x = tf.reshape(x, [1,3,3,1])
w = tf.reshape(w, [2,2,1,1])

out = tf.nn.conv2d(x,w,strds, padding=p)

init = tf.global_variables_initializer()
sess = tf.InteractiveSession()
sess.run(init)
array = out.eval()

print(array)
```
- if p = 'VALID'
>> [6 8; 12 14] 

- if p = 'SAME'
>> [6 8 3; 12 14 6; 7 8 9]  

- if p = 'VALID', strides = [1,1,2,1]
>> [6; 12]  

- if p = 'SAME', strides = [1,1,2,1]
>> [6 3; 12 6; 7 9]  

- Activation Function
  - e.g. relu(x) = max(0, x)
  - in python: 
    - out = tf.nn.conv2d(x,w,strds, padding=p)
    - out = tf.nn.relu(out)

---
Pooling Layer
===
### tf.nn.max_pool(value, ksize, strides, padding, data_format='NHWC', name=None)

- Args.
  - value: X
  - ksize: size of input window
  - strides: same as conv2d
  - padding: same as conv2d
  - data_format, name
- Return
  - pooled sized output
  - e.g. pool= 2\*2 pool, pool([1 2;3 4])--> [1]

### Example
- x = tf.constant([[1., 2., 3., 4.,], [5., 6., 7., 8.], 
[9., 10., 11., 12.], [13., 14., 15., 16.]])
- ksize = [1,2,2,1]
- strides = [1,2,2,1]
- padding = p ('VALID' or 'SAME')
```python
x = tf.constant(
   [[1., 2., 3., 4.,], 
    [5., 6., 7., 8.], 
    [9., 10., 11., 12.], 
    [13., 14., 15., 16.]])

x = tf.reshape(x, [1,4,4,1])

out = tf.nn.max_pool(x,[1,2,2,1],[1,2,2,1], padding=p)

init = tf.global_variables_initializer()
sess = tf.InteractiveSession()
sess.run(init)
array = out.eval()

print(array)
```
- if p = 'VALID'
>> [6 8; 14 16]

- if p = 'SAME'
>> [6 8; 14 16]  # don't have to be same as of VALID

- Pooling doesn't use activation function

---
Fully-Connected(FC) Layer
===

- Fully connect the input to weights
- Hope that all the features are mapped to the node in FC layer
- e.g. X: input, X_FC = fully connected output, W_FC = FC weights
  - X: size=m
  - X_FC: size=n
  - W_FC: m by n
  - X_FC = matmul(X, W_FC)

- Activation Function 
  - e.g. sigmoid(x) = 1/(1+exp(-x))
  - in python: 
    - X_FC = tf.matmul(X, W_FC)
    - out = tf.sigmoid(X_FC)

---

Reference
===
- tensorflow.org
- http://cs231n.github.io/convolutional-networks/#overview
- https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/